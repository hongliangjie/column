---
layout: post
title: AIStats 2017文章精读（五）
categories: 读论文
---

我们在这里对[AIStats 2017](http://www.aistats.org/)文章Communication-Efficient Learning of Deep Networks from Decentralized Data进行一个简单的分析解读。

[全文PDF](http://proceedings.mlr.press/v54/mcmahan17a/mcmahan17a.pdf)

[文章附加信息](http://proceedings.mlr.press/v54/mcmahan17a/mcmahan17a-supp.pdf)

这篇文章的作者群来自Google。文章的核心内容讲的是一个非常有实际意义的问题，那就是在分布式网络的情况下，如何构建合理的机器学习框架。这里说的分布式网络，指的是类似于手机网络这样的系统，用户有不同的数据集合（按照统计意义来说，通常是非IID的），并且这里面主要的陈本是通信陈本，而非计算陈本。传统的设置是不同的分布的数据可能是均匀IID的，而作者们认为在现实情况下，这是很难达到的一种状态。这里面还需要考虑的一些情况就是，如果作为手机客户端的话，每天能够参与优化模型的时间和次数都是有限的（根据电量等因素），因此如何设计一套有效的优化方案就显得非常必要。

这篇文章提出的方案其实非常简单直观。算法总共有三个基本的参数，C（0到1）控制相对有多少数量的客户端参与优化，E控制每一轮多少轮SGD需要在客户端运行，B是每一轮的Mini-Batch的数目大小。算法的思路是：

*  每一轮都随机选择出C那么多的客户端
*  对于每个客户端进行Mini-Batch的大小为B，轮数为E的SGD更新
*  对于参数直接进行加权平均（这里的权重是每个客户端的数据相对大小）

文章对这里的最后一步进行了说明。之前有其他研究表明，如何直接对参数空间进行加权平均，特别是Non-Convex的问题，会得到任意坏的结果。这篇文章里，作者们对于这样的问题的处理是，让每一轮各个客户端的起始参数值相同（也就是前一轮的全局参数值）。这一步使得算法效果大幅度提高。

文章在一系列的数据集上做了大量的实验，基本上都是基于神经网络的模型，例如LSTM，CNN等。效果应该说是非常显著和惊人，绝大多数情况下，提出的算法能够在大幅度比较小的情况下，达到简单SGD很多轮才能达到的精读。

虽然这篇文章提出的算法简单可行，并且也有不错的实验结果。但是比较令人遗憾的是，作者们并没有给出更多的分析，证明这样做的确可以让参数达到全局最优或者局部最优。
这篇文章对于大规模机器学习有兴趣的读者可以精读。
